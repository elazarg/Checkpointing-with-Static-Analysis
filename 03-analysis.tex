\section{Analysis}
\label{sec:analysis}

Our framework performs precise static analysis of Python programs within a tractable subset tailored to numerical computing. This enables advanced source-level transformations such as minimal checkpointing, demonstrated here as our main case study.

\subsection{Scope and Assumptions}
\label{sec:scope}

Our framework targets a practical subset of Python programs, chosen to balance analytical precision with tractable implementation.  
We disallow dynamic code evaluation, require statically resolvable function calls, mandate explicit instantiation for generics, and adopt a simplified aliasing model for NumPy arrays.  
These constraints, common in idiomatic numerical code, allow us to model control flow, data flow, and heap mutation soundly.

Several trade-offs shape the design: we abstract away array shapes, use a wildcard field for unknown collection elements, and leverage literal and variadic generics for precise yet extensible typing.  
Soundness depends on accurate type and effect annotations for external library calls.  

A full statement of these assumptions, including justification and implications, is given in Appendix~\ref{appendix:assumptions}.

\subsection{Intermediate Representation}
Our first step is to lower CPython bytecode to a \emph{register-based three-address code (TAC) intermediate representation} (Appendix~\ref{appendix:ir}), making data flow explicit. For example:
\begin{lstlisting}[language=Python]
# Python
x = f(y) + g(z)

# TAC
$1 = f(y)
$2 = g(z)
x  = $1 + $2
\end{lstlisting}
This removes implicit stack effects, ensures every instruction has explicit inputs and outputs, and simplifies dataflow reasoning.

\subsection{Overview of the Analysis Framework}

Our static analysis framework is built upon three core analyses: Liveness Analysis, Type Analysis, and Pointer and Dirty Analysis. These operate collaboratively within a \emph{reduced product domain}, meaning the insights from each analysis continuously inform and refine the others. This iterative refinement is crucial for overcoming the inherent imprecision that would arise if these analyses were run in isolation, allowing us to achieve a highly precise understanding of Python program behavior. The overall pipeline is visualized in Figure~\ref{fig:overview}.

\begin{figure}[t]
    \centering
    \input{pipeline.tex}
    \caption{Analysis pipeline. Python source is compiled to bytecode, lowered to TAC, and analyzed in three interacting domains. At loop boundaries, the intersection of live and dirty variables forms the minimal checkpoint set.}
    \label{fig:overview}
\end{figure}

\subsection{Liveness Analysis}

\textbf{Liveness Analysis} is a classical dataflow analysis performed backward through the program's control flow graph. Its primary role is to identify which variables (and the data they refer to) might be read or used in the future before they are overwritten. This "liveness" information is critical for discerning which parts of the program's state are truly relevant at any given point, particularly at loop boundaries. By identifying data that is no longer live, we can safely discard it for optimization purposes, such as reducing checkpoint size.

\subsection{Type Analysis}
\label{sec:typesystem}

Our \textbf{Type Analysis} system assigns a precise semantic type to every variable and abstract object in the program. Unlike simple type checkers, our system is designed to capture subtle aspects of Python's dynamic typing behavior in a static context. Key features that enhance its precision and utility include:

\begin{itemize}
    \item \textbf{Literal Values:} The system can track specific constant values, such as an exact integer (\texttt{Literal[3]}) or a precise string (\texttt{Literal["mean"]}). This enables highly accurate reasoning about operations where the outcome depends on these specific values, allowing us to resolve method calls or attribute accesses precisely at analysis time.
    \item \textbf{Generic Types:} We explicitly model generic collections and data structures, like \texttt{list[int]} (a list containing only integers) or \texttt{numpy.ndarray} (a NumPy array). This allows the analysis to understand the types of elements within containers, which is fundamental for numerical computing where array contents are critical.
    \item \textbf{Effect Annotations:} A unique and vital aspect of our type system is the inclusion of \emph{effect annotations} on function types. These annotations explicitly describe a function's side effects on the heap, even for built-in Python functions or external libraries where we don't have source code. This is crucial for tracking mutations and heap changes:
    \begin{itemize}
        \item \texttt{@new}: Indicates the function creates and returns a fresh, new object (e.g., \texttt{list()} or \texttt{np.array()}).
        \item \texttt{@update}: Signifies that the function modifies one of its input arguments (e.g., \texttt{list.append()} mutates the list it's called on).
        \item \texttt{@pure}: Means the function has no side effects on the heap; it only reads its inputs and returns a value (e.g., \texttt{len()}).
    \end{itemize}
\end{itemize}
This detailed type information is fundamental for distinguishing between immutable data (which cannot be "dirty") and mutable state, providing the foundational basis for tracking all heap changes. While our system supports more complex features like union types and overloaded functions (Appendix~\ref{appendix:typesystem}), these core elements are most impactful for our analysis goals.

\paragraph{Heap-indexed maps.}
We use three primary abstract maps over the set of abstract objects~$\mathcal{O}$.
Let $\mathcal{F}$ be the set of abstract field identifiers, consisting of
all possible attribute names plus the wildcard~\texttt{*} denoting collection
elements. In our simplified aliasing model, \emph{all} element accesses
into lists, tuples, NumPy arrays, and other sequence-like containers are
represented by the wildcard~\texttt{*}, rather than distinct integer indices.
This means the analysis does not distinguish between different positions
within the same collection.
Let $\mathcal{T}$ be the type lattice from \S\ref{sec:typesystem}.

\begin{itemize}
    \item \textbf{Pointer graph}:
    \[
        G : \mathcal{O} \to (\mathcal{F} \to 2^{\mathcal{O}})
    \]
    where $G[o][f]$ is the set of abstract objects that field $f \in \mathcal{F}$ of object~$o$
    may point to.  

    \item \textbf{Type map}:
    \[
        T : \mathcal{O} \to \mathcal{T}
    \]
    where $T[o]$ is the abstract type of object~$o$.

    \item \textbf{Dirty map}:
    \[
        D : \mathcal{O} \to 2^{\mathcal{F}}
    \]
    where $D[o]$ is the set of fields of object~$o$ that have been (abstractly) written since
    the last checkpoint boundary.  
    A nonempty $D[o]$ means $o$ is ``dirty,'' but dirtiness is tracked \emph{per-field}.
\end{itemize}

Here $2^X$ denotes the powerset of~$X$, i.e., the set of all subsets of~$X$.

\subsection{Pointer and Dirty Analysis}

The \textbf{Pointer and Dirty Analysis} component provides a detailed understanding
of how variables refer to objects in memory and which parts of that memory have been modified.
It operates over the heap-indexed maps $G$, $T$, and $D$ defined above:

\begin{itemize}
    \item \textbf{Abstract Objects:}
    We represent runtime objects as elements of $\mathcal{O}$, rather than tracking
    concrete memory addresses. These can correspond to allocations at specific program
    points, function parameters, immutable values, or special scope objects such as
    \texttt{LOCALS} and \texttt{GLOBALS}.  
    Immutability is determined statically from $T[o]$ (the type of~$o$): 
    built-in scalars, tuples of immutable types, pure functions, and certain type objects
    are classified as immutable, while containers and most user classes are not.
    Immutable objects are mapped to canonical representatives and never appear in $D$
    (the set of dirtied fields), eliminating spurious updates.

    \item \textbf{Pointer Graph:}
    $G[o][f]$ is the set of objects that field~$f$ of~$o$ may point to.
    This graph encodes aliasing: if two variables map via $G$ to a common object,
    a modification through one is visible through the other.

    \item \textbf{Field-Sensitivity:}
    Because $G$ is defined over individual fields $f \in \mathcal{F}$, 
    the analysis can distinguish updates to different fields or collection elements.
    For example, if $f = \texttt{*}$ (wildcard), then $G[o][\texttt{*}]$ denotes
    all objects that may be stored in collection~$o$.

    \item \textbf{Dirty Tracking:}
    $D[o]$ is the set of fields of object~$o$ that have been written since the last
    checkpoint. When a field~$f$ is updated, $f$ is added to $D[o]$, and this information
    is propagated through all aliases in $G$.
    If $T[o]$ is immutable, then $D[o]$ is always empty.
\end{itemize}

By explicitly tracking $G$ (pointer relationships), $T$ (types), and $D$ (dirtied fields),
we obtain a precise view of memory layout, aliasing, and mutation patterns,
which is indispensable for our checkpointing strategy.

\subsection{Interactions between Analyses}

The power of our framework comes from the continuous interaction and refinement among
Liveness Analysis, Type Analysis, and Pointer/Dirty Analysis within the reduced product domain.
Information flows between these components as follows:

\begin{itemize}
    \item \emph{Type $\rightarrow$ Dirty:}
    If $T[o]$ indicates that object~$o$ is immutable, then $D[o]$ remains empty, even if $o$ appears on the left-hand side of an assignment. This prevents immutable objects from being spuriously marked dirty.

    \item \emph{Dirty $\rightarrow$ Liveness:}
    If $D[o] \neq \varnothing$ (some fields of~$o$ are dirty) but the Liveness Analysis shows that no live variable can reach~$o$ through $G$ at a given program point, then $o$ can be excluded from the checkpoint set.

    \item \emph{Liveness $\rightarrow$ Pointer:}
    When a variable~$v$ is dead, all edges $G[o_v][f]$ for $f \in \mathcal{F}$ are removed, where $o_v$ are the abstract objects referenced by~$v$.

    \item \emph{Pointer $\rightarrow$ Type:}
    For dynamic dispatches such as method calls on variable~$v$, the possible receiver objects $o \in G[o_v][f]$ constrain the possible types $T[o]$.
    This narrowing refines effect annotations (e.g., whether a method updates a field) and thus leads to more accurate updates to $D$.
\end{itemize}

By exchanging information through $G$ (pointer graph), $T$ (type map), and $D$ (dirty map),
each domain avoids overly conservative approximations that would otherwise inflate
the computed checkpoint set in Equation~\eqref{eq:checkpointset}.

\subsection{Checkpoint Minimization}
Naïvely checkpointing every live variable at loop end wastes space and time on data
that is either unchanged or easily recomputed. Our key observation is that only
variables \emph{both} live and dirty need to be saved:
\begin{equation}
    \label{eq:checkpointset}
    \mathsf{CheckpointSet} \;=\; \mathsf{Live} \;\cap\; \mathsf{Dirty}.
\end{equation}
Here:
\begin{enumerate}
    \item \textbf{Live} comes from liveness analysis: variables (and reachable heap)
    used in later iterations.
    \item \textbf{Dirty} comes from pointer/dirty analysis: variables whose reachable heap
    has been modified since the last checkpoint.
\end{enumerate}
In the implementation, the dirty set is computed at loop end by identifying dirty roots
via alias-closure over the pointer graph, then intersected with the live set at loop entry.
This ensures that objects whose last use occurs before the loop boundary, even if dirtied,
are excluded.

\paragraph{Example (K-Means).}
In each iteration of the K-Means clustering algorithm:
\begin{itemize}
    \item \texttt{assignments}: recomputed each time $\Rightarrow$ dead at loop end.
    \item \texttt{points}: input data, never modified $\Rightarrow$ live but not dirty.
    \item \texttt{centroids}: updated and used in the next iteration $\Rightarrow$ live and dirty.
\end{itemize}
In our notation, if $o_c$ is the abstract object for \texttt{centroids}, then
$o_c \in \mathsf{Live}$ and $D[o_c] \neq \varnothing$, so it appears in
$\mathsf{CheckpointSet}$ by Equation~\eqref{eq:checkpointset}.
The intersection therefore singles out \texttt{centroids} as the only array to persist,
eliminating large transient data from checkpoints.

\subsection{Instrumentation and Execution}

The analysis results drive an automated transformation pass that inserts code at statically determined loop boundaries:
\begin{itemize}
    \item \textbf{Restore:} At loop entry, load the most recent saved state if recovering from failure.
    \item \textbf{Save:} At loop exit, serialize only the computed $\mathsf{CheckpointSet}$ (e.g.\ via \texttt{pickle}).
\end{itemize}
This preserves the original program’s structure and semantics while drastically reducing checkpoint cost. Because checkpoint locations and contents are statically determined, no runtime scanning or dependency tracking is needed.
