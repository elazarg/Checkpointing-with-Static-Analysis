

% \section{Introduction: Systems}
% Cloud computing has become a foundation of modern infrastructure, offering flexible and scalable compute resources. However, as the number of compute units per node increases, so does the likelihood of individual node failures. Unexpected faults in long-running or resource-intensive computations can lead to costly recomputation and lost work.

% To mitigate this, systems commonly deploy fault-tolerance mechanisms based on checkpointing --- capturing the program’s state at designated points to enable recovery after failure. Traditional checkpointing is typically implemented at the system level, treating applications as black boxes. Tools like CRIU snapshot the entire Linux process, including memory, file descriptors, and registers; virtual machine snapshots go further, capturing entire guest operating systems.

% While broadly applicable, system-level checkpointing trades generality for semantic precision: tools like CRIU or Firecracker snapshots capture \emph{all} modified memory, regardless of whether that data is actually required to resume correct execution. While recent optimizations, such as page differencing~\cite{} or cache-line tracking~\cite{}, can reduce the volume of data captured, they cannot avoid saving large, short-lived buffers that will be recomputed in the next iteration.

% In contrast, deep learning frameworks like PyTorch implement semantic checkpointing by explicitly persisting model parameters and optimizer state while discarding transient intermediates such as activations and gradients. These benefits, however, come at the cost of full framework adoption: applications must use specialized data types, conform to rigid execution models, and often restructure core logic.

% This leaves a third class of programs underserved: numerical Python code written with general-purpose libraries like NumPy and SciPy. These programs frequently exhibit structured, iterative patterns (e.g., optimization loops, numerical solvers, matrix manipulation pipelines), yet lack native support for semantic checkpointing. We address this gap with a static analysis that infers a minimal checkpoint frontier --- persisting only the live objects needed to resume execution --- without requiring annotations or migration to a managed runtime.

% Application-aware checkpointing offers an alternative: preserving only the program state that influences future computation. While this idea dates back decades~\cite{li1990catch}, and has recently resurfaced~\cite{kim2024lact}, it has seen virtually no adoption in dynamic languages like Python. The difficulty lies in the language’s dynamic semantics --- runtime typing, reflection, closures --- which make sound static analysis difficult.

% Yet many numerical workloads written in Python, particularly in classical machine learning, avoid these problematic features. Programs built with NumPy and SciPy often follow deterministic, iterative patterns, increasingly use type hints, and avoid reflection. This creates an opportunity: to checkpoint such programs efficiently, from within the program itself, without treating it as a black box.

% This paper presents an application-aware checkpointing approach tailored to this specific setting: structured, well-typed numerically intensive Python programs with a designated main loop. Our system identifies the minimal subset of Python objects that must be persisted to resume execution correctly at a designated loop boundary. It combines static type analysis, points-to analysis, and liveness analysis, supported by lightweight effect annotations on library functions (e.g., "updates this", "returns new").

% For example, in k-means clustering, only the array of centroids must persist across iterations. Traditional checkpointing captures the full working set: cluster assignments, distance matrices, temporary buffers --- most of which are recomputed in the next iteration. Our system identifies and stores only what is semantically necessary. The same applies to feature selection heuristics and graph search algorithms, which we also evaluate.

% We implemented our approach and applied it to several nontrivial programs. Our technique stores checkpoint data as raw Python object snapshots with no compression. Nonetheless, we achieve checkpoint sizes 2--5 orders of magnitude smaller than CRIU and VM-based methods, even when those baselines use incremental or diff-based compression. The savings are due entirely to semantically guided state selection, not data encoding.

% It is important to clarify the scope and limitations of our work. We do not attempt to checkpoint arbitrary Python programs. Our approach assumes (1) no use of reflection or dynamic evaluation, (2) no closures or higher-order control constructs, (3) statically well-typed code, and (4) trusted effect annotations for external library functions. These constraints are satisfied by many numerical computing programs but are not universally applicable.

% Moreover, our evaluation focuses on checkpoint size only. We do not measure recovery time or verify that recovery from every checkpoint produces semantically equivalent outputs. Our system is designed to support correct resumption, defined as restoring the program to a state from which it produces the same observable behavior. Demonstrating and formally verifying this remains future work.

% Despite these limitations, our results demonstrate that semantically guided checkpointing can be both practical and highly efficient in a domain where system-level checkpointing is increasingly infeasible. This enables frequent and low-overhead checkpoints in Python-heavy compute environments --- bringing fine-grained fault tolerance within reach for a class of programs that have historically lacked support.

% \section{Old Introduction}
% \aitem{Set-up: the reality of HPC, imminent crashes may lead to loss of valuable compute time and/or streaming data.}

% Cloud computing has become an essential component of modern computing infrastructure, providing a flexible and scalable platform for businesses and individuals to store and process their data.
% As the number of units in a compute cluster increases, the likelihood of failure of one or more units also increases. 
% In cases of unpredicted faults, expensive compute time invested in the failed component may be lost.
% To mitigate the risk, fault-tolerance mechanism are usually deployed.
% This typically involves backing up the computation state and offloading it into a separate storage, from which the computation can be resumed in case the main compute component fails.


% Synchronization of replicated data and program state is a cornerstone
% requirement for systems designed to guarantee application
% fault-tolerance 
% and high-availability \todo{mention high-availability?}
% by maintaining multiple copies across failure
% domains. Replication involves propagating writes from one data
% structure (or memory space) to another in a consistent manner.


% %\aitem{*DW: can we think beyond crashes, e.g., time travel}

% \aitem{Existing solutions: system checkpointing via snapshots based on virtualization. Their downside: write amplification due to not knowing
% which pieces of memory have changed (*SI: actually this can be overcome at least theoretically by tracing dirty pages/cache lines like in your SoCC paper?) and more importantly which memory has changed the defines "progress of work",
% which causes heavy traffic of
% scratch buffers of intermediate computations.
% }

% One of the common techniques to obtain such replication synchrony is \emph{checkpointing}: capturing the state of the computation at key points during its execution
% in a way that will allow recovering this state in case of failure.
% One possible way to do it is to run the computation in a virtualized environment and capture the memory state of the VM, and transferring the changes done on the memory space to a passive/backup machine.
% This, of course, has the disadvantage of high volumes of data transfer. This problem escalates as we scale with the number of VMs that run in parallel on the same physical node, {\it e.g.}, in the case that hundreds of thousands of light VMs running in parallel~\cite{Firecracker}, making traditional checkpointing infeasible 
% due to memory, communication and storage constraints, thus fault-tolerance is abandoned in such cases\todo{substantiate}.   
% Our aim in this paper is to offer a more efficient approach that provides the same availability properties with lower overhead, practically enabling checkpointing in such environments. 

% \SI{say something about epochs}
% % In the context of machine learning, an epoch refers to one complete pass through the entire dataset during the training process. During each epoch, the model's parameters are updated as it learns by adjusting to minimize the loss function. This iterative process, repeated over multiple epochs, allows the model to gradually improve its performance as it continues to optimize its parameters based on the data.


% A trivial solution for checkpointing would be to take a full snapshot of the memory space, either for the entire machine or for a particular process. This approach is very wasteful as most of the memory space does not change between epochs. A straightforward improvement would be to identify only the parts of the memory that changed from the last snapshot, and store the delta as part of the checkpoint.
% In order to achieve this efficiently, one can leverage system-level support such as PTE dirty bits~\cite{glausch16pml} 
% which allow tracking changes at the granularity of memory pages.
% One prominent disadvantage is that pages are relatively large (4KiB), and is has been shown~\cite{MoshikSOCC22} that using a finer granularity such as cache lines (16 machine words) significantly reduces the size of the replicated data to be synchronized. While tracking changes using finer granularity may lead to smaller snapshots, such an approach has the limit that every byte what was written has to be stored in the checkpoint even if it has no influence on the rest of the execution. 

% \aitem{Proposed solution: application-aware checkpoints that intelligently persist runtime data that is needed for future recovery.}

% All the techniques described above are system-level; they treat the program as a black-box, and only observe the writes that is performs, ignoring control and data flow.
% %making important data for the program state and temporary data indistinguishable.
% We propose a different approach that takes advantage of application-level knowledge by observing the source code, and leverages this knowledge to reduce the snapshot sizes.
% In particular, it can distinguish between data that is important for the remainder of the program's execution, and temporary data that has outlived its purpose.
% This is mostly beneficial in YYY algorithms, where it is common to use an auxiliary data structure as ``scratch space'' for performing some computation, after which the auxiliary data is scrapped.

% For example, in machine learning, certain model parameters must be preserved throughout the training process. Take k-means clustering as an example: the 
% algorithm consists of a simple loop that updates an array of $k$ centroids, where at each iteration, some computation involving the mean of the centroids is used to better fit the centroids to the obtained clusters at each iteration.
% While the vectors of the centroids (and the iteration counter) are updated on each iteration, the rest of the data used is ephemeral and is discarded once the centroids have been updated.
% It would be wasteful to store the scratch data in the snapshop along with the centroids, because the former is no longer read in subsequent iterations.
% It is desirable to detect future uses of data elements and only store ones that are viable, thus significantly reducing the memory footprint of the snapshot.


% %\aitem{*DW: application-aware vs. system check-pointing ?}

% \aitem{Target applications: epoch-based algorithms, including (but not limited to) classical ML models. Mostly numerical processing on large arrays, frequently in dynamic languages such as Python.}

% Numerous applications, especially those in the field of machine learning, often rely on loops. Consider the traditional training loop in machine learning, where the process iterates over several epochs. In each epoch, a specific batch of samples is loaded, and the algorithm updates the weights of the parameters according to the designated loss function. Such training loops can run for extensive periods, and efficiency becomes paramount.
% Should a failure occur during an iteration, restarting the entire training process from the beginning would be wasteful and highly expensive in terms of computation time. Traditional checkpointing methods might allow for occasional saving of the model, yet this approach can still lead to significant waste.

% Our goal is to streamline this process through the use of static analysis, as we develop a method that automatically identifies the variables changing during an iteration. By storing and transmitting only the updates of these particular variables, we can create a checkpointing mechanism that maintains a much lower memory footprint. This approach not only minimizes waste but also enables high-frequency checkpointing, making the entire process more efficient and resilient against potential failures.

% \aitem{Challenges: static identification of runtime memory accesses, esp. in said dynamic languages where dynamic memory allocation is dominant.}

% Our approach of using static analysis to identify the variables that change during an iteration requires handling several challenges. One of the main challenges is static identification of
% memory objects that are allocated and written to by the program and would be needed for the remainder of the computation.
% In most implementations, esp. in dynamic languages (where memory allocation is dominant), many temporary objects are allocated for a single computation, and large amounts of scratch space may be used and then discarded.
% Saving these itinerant\todo{?} objects in the snapshot would be redundant and lead to much higher overheads.

% Specifically, we are targeting Python implementations of ML algorithms.
% Tracking pointers in dynamic languages such as Python is challenging because a library call may either allocate or not allocate a new object depending on the types of the variables, which are unknown during analysis.
% For this purpose, our pointer analysis technique must be coupled with an analysis of the possible types.


% \aitem{System overview: (without going too much into technical details) a combination of type analysis, points-to analysis, and liveness analysis, based on Abstract Interpretation with some required information transfer between the different analyses.}

% We present a mechanism for statically analyzing Python programs
% where the user has tagged the main loop via an annotation.
% The analysis is applied to the body of the loop.
% In order to infer which variables point to objects that may be
% accessed in a subsequent iteration, we use a liveness analysis
% combined with a points-to analysis to overapproximate the heap
% structure.
% An object is ``live'' \todo{think of a different term} at the end
% of the loop if it is reachable from one of the live variables.
% The checkpointing strategy is therefore to persist all the live
% objects at the end of the iteration.
% We chose Python as our setting because many machine learning algorithms are implemented in Python 
% using popular libraries such as numpy and scipy.
% We focus on machine-learning implementations that typically consist of a training loop in which
% at every iteration adjusts the parameters closer to an optimum, such as minizing a loss function.
