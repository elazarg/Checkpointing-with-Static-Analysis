\section{Related Work}
\label{sec:related}

\paragraph{From feasibility to structural analyzability.}
Early efforts showed that static reasoning about Python is possible: Starkiller~\cite{salib2004starkiller} performed whole-program type inference, RPython~\cite{ancona2007rpython} and PyPy defined analyzable sublanguages for ahead-of-time compilation, and Gorbovitski et al.~\cite{gorbovitski2010alias} demonstrated practical alias analysis. These ambitious systems were early signs of what later became a trend. The turning point came with gradual typing~\cite{siek2007gradual,vitousek2014design}: tools like Mypy~\cite{mypy}, Pytype~\cite{pytype}, and Pyre~\cite{pyre} brought type annotations into production Pytho. Building on this foundation, abstract interpretation frameworks now track values and effects across modules, including native C extensions~\cite{fromherz2018static,monat2021static,monat2021multilanguage}, while practical call-graph construction handles dynamic features at scale~\cite{salis2021pycg,rakamnouykit2024potohybridandersenspointsto}. Empirical studies confirm that type annotations improve analyzability in practice~\cite{bence2021unambiguity}. Dynamic ecosystems beyond Python show the same evolution, for example JavaScript~\cite{jensen2009type,kashyap2014jsai} and Ruby~\cite{furr2009static,kazerounian2021simtyper}. Together, these developments reflect a broader shift: Python usage increasingly favors disciplined, analyzable patterns, making automated transformations practical.

\paragraph{Compiler-guided, application-level checkpointing.}
Compiler assistance for checkpointing has a clear lineage. CATCH~\cite{li1990catch} saved only state proven live across restart and reduced overhead substantially. Application-level frameworks in HPC, including C3~\cite{bronevetsky2004application} and CPPC~\cite{rodriguez2010cppc}, instrumented programs to persist the variables identified by compile-time data flow rather than snapshotting whole processes. Later work refined this idea in new settings: structure-aware schemes for loop and array codes~\cite{elnawawy2017efficient}, storage-aware optimizations that leverage non-volatile memory~\cite{kannan2013optimizing}, and compiler-directed incremental checkpointing for preemption~\cite{ji2022compiler}. Recent results push liveness down to array regions in intermittent systems, excluding dead elements and reducing writes at compile time~\cite{kim2024lact}. System-level tools such as BLCR~\cite{hargrove2006berkeley}, DMTCP~\cite{AnselAryaCooperman2009DMTCP} and CRIU~\cite{CRIUProject}, including in-memory CRIU and VM-delta methods, remain process- or VM-wide by design~\cite{venkatesh2019fast,MoshikSOCC22}.

\paragraph{Convergence in Python.}
These trajectories meet in scientific and numerical Python, where workloads increasingly exhibit analyzable patterns: type annotations, predictable control flow, and stable libraries~\cite{bence2021unambiguity,PEP484}. Our work exploits this convergence, showing that compiler techniques for reliability --- not just performance --- can extend to dynamic languages.
