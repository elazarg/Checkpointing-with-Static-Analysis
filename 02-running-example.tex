
\begin{figure}[t]
\begin{tikzpicture}
\node(py)[scale=0.9,inner ysep=0] {
\begin{lstlisting}[language=python]
import numpy as np

def kmeans(X:np.ndarray, k:int, niters:int):
    # initialization
    centroids = X[np.random.choice(X.shape[0], k)]
    clusters = list[list[int]]()
    for i in range(niters):
        # loop body
        clusters = [list[int]() for _ in range(k)]
        for sample_i in range(len(X)):
            v = X[sample_i] - centroids
            r = np.linalg.norm(v, None, 1).argmin()
            clusters[r].append(sample_i)
        new_centroids = np.array([X[cluster].mean(0)
                             for cluster in clusters])
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids
    # finalization
    return compute_labels(clusters, X.shape[0])
\end{lstlisting}
};
\node(pseudo)[right=0 of py.north east,
anchor=north west, scale=0.8] {
\begin{minipage}{9cm}
\begin{algorithmic}
\Function{k-means}{$X$, $k$, $n$}
\State $\mathit{centroids} \gets \textrm{choose}~
  \{C\subseteq X \mid |C|=k\}$
\Loop ~(max $n$ iterations)
  \State $\mathit{clusters} \gets \big\{
    \{p\in X \mid p \textrm{~is closest to~} c\}$ 
  \State \hspace{3.5cm} $~\big|~ c\in \mathit{centroids} \big\}$
  \State $\mathit{centroids}' \gets \{\mu(C) \mid
    C \in \mathit{clusters}\}$
  \If {$\mathit{centroids}' \approx \mathit{centroids}$}
    \State \textbf{break}
  \Else
    \State $\mathit{centroids} \gets \mathit{centroids}'$
  \EndIf
\EndLoop
\EndFunction
\end{algorithmic}
\end{minipage}
};

\node[below=0 of pseudo,inner sep=0,yshift=2mm] {
  \begin{tikzpicture}[>=stealth,
        every node/.style={font=\sffamily,inner sep=2pt}]
    \node(centroids) {centroids};
    \node(X)[above right=0.3 of centroids] {X};
    \node(v)[below right=0.3 of centroids] {v};
    \node(r)[below=0.2 of v] {r};
    \node(clusters)[below=0.3 of r,inner ysep=1pt] {\rule[-.2\baselineskip]{0pt}{.5\baselineskip} clusters};
    \node(new-centroids)[below=0.3 of clusters,inner ysep=1pt] 
         {new\_centroids};
    \draw[->] (X) edge[out=190,in=45] (centroids);
    \draw[->] (centroids) edge[out=-45,in=170] (v);
    \draw[->] (X) edge[out=-45,in=45] (v);
    \draw[->] (v) -- (r);
    \draw[->] (r) -- (clusters);
    \draw[->] (clusters) -- (new-centroids);
    \draw[->] (new-centroids)
      edge[out=180,in=-115] (centroids.-160);
  \end{tikzpicture}
};
\end{tikzpicture}
\caption{\label{lst:code-kmeans}
K-Means clustering with $k$ clusters, implemented in Python using \lstinline|numpy|. \hspace{0.5em}To the right:\\
\strut\qquad algorithm pseudo-code (for reference and as a reading aid), and a data-flow diagram of the program.}
\end{figure}

\subsection{Running Example: K-Means Clustering}
\label{sec:running-example}

We illustrate our checkpointing approach using the classical \emph{K-Means clustering} algorithm~\cite{macqueen1967multivariate}.
Clustering is a classical machine learning problem defined as partitioning a set $X$ of data points (real-valued vectors) into $k$ clustered subsets.
The K-Means algorithm works by finding $k$ points, called
\emph{centroids}, where the $i$'th cluster consists of all points in $X$ that are closer to the $i$'th centroid that
to all other centroids.
This widely used iterative method, starting with a random set of centroids and repeatedly, gradually, improving the solution until the process converges.

\smallskip
Each iteration proceeds as follows:
\begin{enumerate}
    \item \textbf{Cluster assignment:} assign each data point to its nearest centroid, forming $k$ clusters.
    \item \textbf{Centroid update:} compute new centroids as the mean of each cluster.
    \item \textbf{Convergence check:} if centroids have not significantly changed from previous iteration, the algorithm terminates.
\end{enumerate}

This style of computation is a natural fit for checkpointing: each iteration overwrites the previous cluster assignments and recomputes temporary data structures from scratch. Only the \texttt{centroids} array must persist across iterations. However, conventional checkpointing tools like CRIU or VM-based snapshots indiscriminately capture the full resident memory---including all temporary arrays and internal buffers---leading to high storage and I/O overhead.

Our system avoids this inefficiency by identifying the \emph{minimal} state necessary to resume execution after a crash. It does so through a static analysis pipeline that combines:
\begin{itemize}
    \item \textbf{Liveness analysis} to determine which variables are needed for future execution.
    \item \textbf{Points-to analysis} to track object references and heap structure, in order to map memory
location that are reachable from each variable.
    \item \textbf{Dirty analysis} to identify which memory locations are actually being modified thoughout the execution.
\end{itemize}

Program variables that are both \textbf{live} and from which \textbf{dirty} memory locations are \textbf{reachable}
are marked for inclusion in the snapshot.
In the case of K-Means, the only variable that has both of these properties is \lstinline|centroids|.
All other variables---the input \lstinline|X|, the \lstinline|clusters| data structure, distance vectors \lstinline|v|, and other intermediate computed values---are either dead or unchanged.

\begin{figure}[t]
\newcommand\blk[1]{%
  \tikz[baseline=(a.base)]
  \node(a)[draw, dashed, dash pattern=on 2pt off 1pt, minimum height=15pt] {\# #1}; }
\newcommand\lstrut{%
  \rule[-.3\baselineskip]{0pt}{1.1\baselineskip}}
\centering
\begin{lstlisting}[language=python]
t = spyte.restore()   # recover saved transaction, if any
(*@\blk{initialization}@*)
t (*@$>\!>$@*) [centroids]
for i in t.iter(range(niters)):
  (*@\blk{\lstrut loop body}@*)
  t (*@$<\!<$@*) [centroids]
(*@\blk{finalization}@*)
\end{lstlisting}
\caption{Automatic checkpoint injection for the k-means loop.}
\label{fig:checkpoint-injection}
\end{figure}

Following these conclusions derived from the analysis, \spyte generates an instrumented version of the function
with injected checkpointing operations, shown in \autoref{fig:checkpoint-injection}.
A transaction object \lstinline|t| encapsulates the checkpoint API (it is shown here in pseudo-code form to keep the presentation clean and free of distracting implementation details).
The key point is the store operation \lstinline|t|$<\!<$\lstinline|[centroids]|, which updates the snapshot (on disk) with the current value of \lstinline|centroids|, which,
as explained above, is the only variable marked during the analysis phase.
In addition, the transaction keeps track of the iteration counter \lstinline|i|, which is required for resumption if the program crashes and needs to re-run starting from the \lstinline|i|'th iteration.


\paragraph{Language Features and Implementation Twists}

Although K-Means is conceptually simple, its Python implementation highlights several challenges for static analysis in a dynamic language. \si{these sentences use a mixture of terms: features, challenges, general properties. needs cleanup} In particular:
\begin{itemize}
    \item \textbf{Generic data structures.} The variable \texttt{clusters} is a list of lists of integers. In order to enable type-checking and proper tracking of its structure, we require the use of explicit type constructors such as \texttt{list[int]()}. This ensures that our analysis can infer a meaningful and consistent type.
    \item \textbf{Side effects.} Function calls such as \texttt{xs.append(y)} mutate nested list. Our dirty analysis correctly tracks such effects to determine whether the object needs to be persisted. However, in this case, \texttt{clusters} is reconstructed every iteration and does not need to be saved.
    \item \textbf{Effect annotations.} For functions like \texttt{np.array} and \texttt{np.linalg.norm}, we rely on an internal library of effect annotations that describe whether a function allocates a new object, updates its arguments, or returns an alias. For example, \texttt{np.array} is treated as a constructor (``new''), while \texttt{np.mean} is considered pure. These annotations are essential for tracking heap mutations and avoiding conservative over-approximation.
    \item \textbf{Control-flow simplification.} Because Python bytecode is complex, we translate it to a simplified intermediate representation for verification and analysis.
\end{itemize}

Despite the simplicity of the target algorithm, precise analysis demands careful handling of Python's typing and memory model. Our implementation supports this through a combination of expressiveness in the type domain and selective annotation of external APIs. The result is an end-to-end system that enables lightweight, high-frequency checkpointing with no manual specification of which variables to save.
