
\begin{figure}[t]
\begin{tikzpicture}
\node(py)[scale=0.9,inner ysep=0,inner xsep=3mm] {
\begin{lstlisting}[language=python,numbers=left]
import numpy as np

def kmeans(X: np.ndarray, k: int, niters: int):
    # initialization
    centroids = X[np.random.choice(X.shape[0], k)]
    clusters = list[list[int]]()
    for i in range(niters):
        # loop body
        clusters = [list[int]() for _ in range(k)]
        for sample_i in range(len(X)):
            v = X[sample_i] - centroids
            r = np.linalg.norm(v, None, 1).argmin()
            clusters[r].append(sample_i)
        new_centroids = np.array([X[cluster].mean(0)
                           for cluster in clusters])
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids
    # finalization
    return compute_labels(clusters, X.shape[0])
\end{lstlisting}
};
\node(pseudo)[right=0 of py.north east,
anchor=north west, scale=0.85] {
\begin{minipage}{9cm}
\begin{algorithmic}
\Function{k-means}{$X$, $k$, $n$}
\State $\mathit{centroids} \gets \textrm{choose}~
  \{C\subseteq X \mid |C|=k\}$
\Loop ~(max $n$ iterations)
  \State $\mathit{clusters} \gets \big\{
    \{p\in X \mid p \textrm{~is closest to~} c\}$ 
  \State \hspace{3.5cm} $~\big|~ c\in \mathit{centroids} \big\}$
  \State $\mathit{centroids}' \gets \{\mu(C) \mid
    C \in \mathit{clusters}\}$
  \If {$\mathit{centroids}' \approx \mathit{centroids}$}
    \State \textbf{break}
  \Else
    \State $\mathit{centroids} \gets \mathit{centroids}'$
  \EndIf
\EndLoop
\State \Return \ldots
{\color{white} \EndFunction}
\end{algorithmic}
\end{minipage}
};

\node[below=0 of pseudo,inner sep=0,yshift=3mm,scale=0.8,anchor=60] {
  \begin{tikzpicture}[>=stealth,
        every node/.style={font=\sffamily,inner sep=2pt}]
    \node(centroids) {centroids};
    \node(X)[above right=0.3 of centroids] {X};
    \node(v)[below right=0.3 of centroids] {v};
    \node(r)[below=0.2 of v] {r};
    \node(clusters)[below=0.3 of r,inner ysep=1pt] {\rule[-.2\baselineskip]{0pt}{.5\baselineskip} clusters};
    \node(new-centroids)[below=0.3 of clusters,inner ysep=1pt] 
         {new\_centroids};
    \draw[->] (X) edge[out=190,in=45] (centroids);
    \draw[->] (centroids) edge[out=-45,in=170] (v);
    \draw[->] (X) edge[out=-45,in=45] (v);
    \draw[->] (v) -- (r);
    \draw[->] (r) -- (clusters);
    \draw[->] (clusters) -- (new-centroids);
    \draw[->] (new-centroids)
      edge[out=180,in=-115] (centroids.-160);
  \end{tikzpicture}
};
\end{tikzpicture}
\caption{\label{lst:code-kmeans}
K-Means clustering with $k$ clusters, implemented in Python using \lstinline|numpy|. \hspace{0.5em}To the right:\\
\strut\qquad algorithm pseudo-code (for reference and as a reading aid), and a data-flow diagram of the program.}
\end{figure}

\subsection{Running Example: K-Means Clustering}
\label{sec:running-example}

We illustrate our checkpointing approach using the classical \emph{K-Means clustering} algorithm~\cite{macqueen1967multivariate}.
Clustering is a classical machine learning problem defined as partitioning a set $X$ of data points (real-valued vectors) into $k$ clustered subsets.
The K-Means algorithm works by finding $k$ points, called
\emph{centroids}, where the $i$'th cluster consists of all points in $X$ that are closer to the $i$'th centroid that
to all other centroids.
This widely used iterative method, starting with a random set of centroids and repeatedly, gradually, improving the solution until the process converges.

\smallskip
Each iteration proceeds as follows:
\begin{enumerate}
    \item \textbf{Cluster assignment:} assign each data point to its nearest centroid, forming $k$ clusters.
    \item \textbf{Centroid update:} compute new centroids as the mean of each cluster.
    \item \textbf{Convergence check:} if centroids have not significantly changed from previous iteration, the algorithm terminates.
\end{enumerate}

This style of computation is a natural fit for checkpointing: each iteration overwrites the previous cluster assignments and recomputes temporary data structures from scratch. Only the \texttt{centroids} array must persist across iterations. However, conventional checkpointing tools like CRIU or VM-based snapshots indiscriminately capture the full resident memory---including all temporary arrays and internal buffers---leading to high storage and I/O overhead.

Our system avoids this inefficiency by identifying the \emph{minimal} state necessary to resume execution after a crash. It does so through a static analysis pipeline that combines:
\begin{itemize}
    \item \textbf{Liveness analysis} to determine which variables are needed for future execution.
    \item \textbf{Points-to analysis} to track object references and heap structure, in order to map memory
location that are reachable from each variable.
    \item \textbf{Dirty analysis} to identify which memory locations are actually being modified thoughout the execution.
\end{itemize}

Program variables that are both \textbf{live} and from which \textbf{dirty} memory locations are \textbf{reachable}
are marked for inclusion in the snapshot.
In the case of K-Means, the only variable that has both of these properties is \lstinline|centroids|.
All other variables---the input \lstinline|X|, the \lstinline|clusters| data structure, distance vectors \lstinline|v|, and other intermediate computed values---are either dead or unchanged.

\begin{figure}[t]
\newcommand\blk[1]{%
  \tikz[baseline=(a.base)]
  \node(a)[draw, dashed, dash pattern=on 2pt off 1pt, minimum height=15pt] {\# #1}; }
\newcommand\lstrut{%
  \rule[-.3\baselineskip]{0pt}{1.1\baselineskip}}
\centering
\begin{lstlisting}[language=python]
def kmeans(X: np.ndarray, k: int, niters: int):
  t = spyte.restore()   # recover saved transaction, if any
  (*@\blk{initialization}@*)
  t (*@$>\!>$@*) [centroids]
  for i in t.iter(range(niters)):
    (*@\blk{\lstrut loop body}@*)
    t (*@$<\!<$@*) [centroids]
  (*@\blk{finalization}@*)
\end{lstlisting}
\caption{Automatic checkpoint injection for the k-means loop.}
\label{fig:checkpoint-injection}
\end{figure}

Following these conclusions derived from the analysis, \spyte generates an instrumented version of the function
with injected checkpointing operations, shown in \cref{fig:checkpoint-injection}.
A transaction object \lstinline|t| encapsulates the checkpoint API (it is shown here in pseudo-code form to keep the presentation clean and free of distracting implementation details).
The key point is the store operation \lstinline|t|$<\!<$\lstinline|[centroids]|, which updates the snapshot (on disk) with the current value of \lstinline|centroids|, which,
as explained above, is the only variable marked during the analysis phase.
In addition, the transaction keeps track of the iteration counter \lstinline|i|, which is required for resumption if the program crashes and needs to re-run starting from the \lstinline|i|'th iteration.


\subsubsection*{Python: the good parts}

The Python code in \cref{lst:code-kmeans} highlights
several aspects of the ``structuredness'' of real-world 
programs, in light of~\citet{bence2021unambiguity}.
In particular:
\begin{itemize}
  \item \textbf{Type annotations.} The inputs \lstinline|X|, \lstinline|k|, \lstinline|niters| are annotated with types.
This is essential for distinguishing (mutable) objects from (immutable) primitive data.
  \item \textbf{Generic data structures.} The variable \texttt{clusters} is a list of lists of integers. 
While Python lists are heterogeneous, developers prefer and encourage the use of homogeneous types for list elements.
To make such information usable by the analysis, \spyte
expects that the developer annotate allocations of such
generic types.
This is done via the use of explicit type constructors
such as \texttt{list[int]()} (line 9).
This ensures that our analysis can infer a meaningful and consistent type.
    \item \textbf{Effect annotations.} For functions like \lstinline|list.append|, \lstinline|np.array|, and \lstinline|np.linalg.norm|, we rely on an internal library of effect annotations that describe whether a function allocates a new object, updates its arguments, or returns an alias.
For example, \lstinline|np.array| is treated as a constructor (the \tnew effect), while \lstinline|np.linalg.norm| is considered pure (has no effect).
\lstinline|list.append| has an \tupdate effect, mutating the receiver object.
These annotations are essential for tracking heap mutations and avoiding conservative over-approximation.
\end{itemize}

Despite the simplicity of the algorithm used in this running example, precise analysis of the Python implementation and its intricacies demands careful handling of Python's typing and memory model.
Our implementation achieves this through a combination of expressiveness in the type domain and selective annotation of external APIs. The result is an end-to-end system that enables lightweight, high-frequency checkpointing with no manual specification of which variables to save.
