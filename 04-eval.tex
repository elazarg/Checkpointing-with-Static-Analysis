
\begin{table}
\centering
\begin{tabular}{l|r|r|r|r}
\toprule
\textbf{Benchmark} & 
\multicolumn{1}{c}{\textbf{\spyte}} &
\multicolumn{1}{c}{\textbf{Locals}} &
\multicolumn{1}{c}{\textbf{\PROCDIFF}} &
\multicolumn{1}{c}{\textbf{\VMDIFF}} \\
\midrule
\multirow{2}{*}{\texttt{noploop}} & 72 & 75 & \textbf{14} & 17966 \\
 &  & {\footnotesize 1.04$\times$} & {\footnotesize 0.19$\times$} & {\footnotesize 249.52$\times$} \\
\midrule
\multirow{2}{*}{\texttt{pivoter}} & \textbf{226} & 261 & 516 & 23400 \\
 &  & {\footnotesize 1.15$\times$} & {\footnotesize 2.28$\times$} & {\footnotesize 103.48$\times$} \\
\midrule
\multirow{2}{*}{\texttt{kmeans}} & \textbf{270} & 9136 & 51418 & 254554 \\
 &  & {\footnotesize 33.84$\times$} & {\footnotesize 190.44$\times$} & {\footnotesize 942.79$\times$} \\
\midrule
\multirow{2}{*}{\texttt{omp}} & \textbf{3593} & 457358 & 832266 & 130034290 \\
 &  & {\footnotesize 127.29$\times$} & {\footnotesize 231.64$\times$} & {\footnotesize 36191.01$\times$} \\
\bottomrule
\end{tabular}
\caption{Average checkpoint sizes (in bytes) across filtered iterations. Small text shows size relative to \spyte.} \centering
\label{tab:checkpoint-sizes}
\end{table}

\section{Evaluation}
\label{sec:evaluation}
We evaluate our approach to program-guided checkpointing on four Python workloads: \texttt{noploop}, \texttt{pivoter}, \texttt{kmeans}, and \texttt{omp}. These benchmarks capture common iterative patterns in numerical computing and machine learning, with varying degrees of control complexity, memory reuse, and heap allocation behavior. In each, we insert checkpoint instrumentation at the head of the primary loop, allowing recovery from the beginning of any iteration. We measure the size of the persisted state after each checkpoint operation.

\begin{itemize}[leftmargin=*]
    \item \textbf{noploop} is a degenerate  {\color{blue!50!black}
\texttt{for}~\textrm{...}~\texttt{in}~\texttt{range}} loop with no meaningful computation or allocation except the iterator. It serves as a lower bound on checkpointing overhead. We run the loop for 100 steps.

    \item \textbf{pivoter} (26 LOC, 2 out of 7 relevant variables) performs clique enumeration over a sparse graph via depth-first search and backtracking~\cite{jain2020power}. Program state is encoded in sets and counters. Input is a 100-node, 757-edge subgraph extracted from the Enron dataset. The algorithm requires $\sim 47,000$ steps to complete. Due to the computational cost of process- and VM-level checkpointing, we sample checkpoints every 50 or 51 steps (yielding 1,891 checkpoint pairs total), but measure memory differences between consecutive snapshots to estimate per-step deltas. This avoids the cost of capturing all 47,000 steps while maintaining fair comparison granularity.

    \item \textbf{kmeans} (15 LOC, 1 out of 10 relevant variables) implements K-Means clustering on synthetic data~\cite{macqueen1967multivariate}. Each iteration reassigns samples to centroids and updates them. Only the centroid array needs to persist. Data is generated using \texttt{sklearn.datasets.make\_blobs} with $\textit{n\_samples}=1000$, $\textit{n\_features}=2$, $k=5$. The algorithm completes in 20 steps.

    \item \textbf{omp} (42 LOC, 2 out of 21 relevant variables)---Orthogonal Matching Pursuit is a greedy feature selection algorithm~\cite{Pati1993OMP, quinzan2022fast}. At each step, it selects the feature most correlated with the residual and retrains a regression model from scratch. The input is the \texttt{healthstudy} dataset with $k = 35$. The algorithm completes in 35 steps.
\end{itemize}

All benchmarks use fixed data and identical loop structure across checkpointing configurations. Dataset selection was finalized before implementing or evaluating instrumentation.

\begin{itemize}[leftmargin=*]
  \item \textbf{Locals}: Pickling (Python data serialization) all local variables (excluding parameters), regardless of whether they were modified or needed again.
  \item \textbf{\PROCDIFF}: We implemented process-level checkpointing using a custom tool that captures all writable memory regions by reading from \texttt{/proc/self/mem}. We initially attempted to use CRIU for process-level checkpointing, but found its memory overhead dominated the storage costs by orders of magnitude, making meaningful comparison impractical. 
  \item \textbf{\VMDIFF}: VM-level checkpointing based on snapshots and 64-byte memory diffing between iterations. The checkpoints are taken by triggering QEMU memory dumps via QMP commands over TCP.
\end{itemize}

We omit the first iteration of each benchmark (since no diff can be computed). We also omit the final two iterations, which introduce irregularities in the {\PROCDIFF} and {\VMDIFF} baselines.

\paragraph{Notes on Overhead Accounting}
The use of TCP to trigger VM-level checkpoints may introduce a nontrivial fixed overhead per snapshot. While difficult to isolate, we partially control for such effects by comparing deltas between consecutive snapshots rather than absolute snapshot sizes. In contrast, \spyte and {Locals} Python checkpoints use direct pickle serialization. Thus, while our VM-level measurements reflect realistic system-level behavior, they may slightly overestimate true memory diffs compared to language-level checkpointing.

\begin{figure*}[t]
    \centering
  \ifdraft[redacted by \texttt{ifdraft}]\else
    \input{experiments/unifiedplots}
  \fi
  \caption{Memory snapshot sizes across iterations for four benchmarks, comparing process-level, VM-level checkpointing with application-level methods: keeping all local variables, and our method \spyte. Lines show averages over five runs; shaded bands indicate min-max ranges across runs. Y-axis is logarithmic. In \texttt{noploop} (top-left), {Locals} and \spyte values overlap.}
  \label{fig:snapshot-graph}
\end{figure*}

\subsection{Checkpoint Size Comparison} 
\Cref{tab:checkpoint-sizes} reports the mean size in bytes of each checkpointing method after filtering. Our system consistently outperforms both Local variables checkpointing and simulated Process- and VM-level methods. ``\spyte'' refers to the instrumented version produced by our static analysis-based checkpointing. ``\PROCDIFF'' and ``\VMDIFF'' refer to memory differentials at the process and virtual machine levels respectively.

For the \texttt{noploop} benchmark, our analysis incurs a fixed overhead (iterator) that exceeds 
the minimal state changes, making process-level diffing more efficient. This represents a limitation for programs with very small persistent state.

The \texttt{pivoter} benchmark, a recursive clique enumeration program, maintains small internal state (e.g., recursion depth and adjacency buffers) that is genuinely required to resume computation. Our systemâ€™s performance is nearly identical to the process-level memory diff, suggesting that it is near-optimal in this case; in contrast, naively checkpointing all local variables leads to 10x memory overhead. The \texttt{noploop} benchmark confirms the lower bound: our instrumentation adds minor overhead (e.g., iterator state), while process-level diffing captures the true minimal footprint. Comparing \PROCDIFF results between \texttt{pivoter} and \texttt{noploop} reveals that process snapshots can be highly efficient in simple cases, adding as little as 23 bytes of overhead.

In \texttt{kmeans}, \spyte achieves significant reductions: $33.84\times$ smaller than {Locals}, $190.44\times$ smaller than \PROCDIFF, and $942.79\times$ smaller than \VMDIFF. This improvement stems from excluding transient arrays---gradients, distances, and cluster metrics---that are allocated per iteration and dead by the loop's end. The \texttt{omp} benchmark shows even more dramatic gains ($127.29\times$, $231.64\times$, and $36191.01\times$ smaller, respectively) because its large working arrays and regression buffers are reused rather than persisted, allowing our dirty-pointer analysis to exclude them from checkpoints.

Beyond these average sizes, examining checkpoint behavior across iterations reveals interesting temporal patterns. \Cref{fig:snapshot-graph} visualizes the per-iteration checkpoint sizes for each technique across our four benchmarks. The plots reveal distinct patterns in checkpoint behavior. In \texttt{noploop}, \PROCDIFF maintains minimal state except for a notable spike mid-execution (likely from buffer reallocation) while our method tracks closely with {Locals} (showing pure checkpoint overhead).
The \texttt{pivoter} benchmark, with its many iterations, shows periodic bursts in checkpoint sizes across all methods; however, \spyte exhibits notably more stable behavior with less dramatic spikes compared to both {Locals} and \PROCDIFF. This stability is advantageous for network transmission and storage systems where sudden bursts can cause latency issues. Note that each point is a diff of two consecutive iterations, not over the 50-step jumps.
The \texttt{kmeans} benchmark demonstrates relatively constant checkpoint sizes across iterations for all techniques, with our method maintaining significant size advantage consistently throughout execution. Notably, \texttt{omp} reveals a critical scaling difference: while most methods show steadily increasing checkpoint sizes as iterations progress (with slopes greater than 1), \spyte maintains a much gentler slope, meaning our performance advantage actually increases with longer-running computations. Across all benchmarks, \VMDIFF exhibits higher variance between runs (visible in the wider shaded bands), while other techniques show more consistent behavior across different executions, with application-level methods (\spyte and {Locals}) having zero variance. These patterns confirm that our static analysis not only reduces average checkpoint size but also provides more predictable and stable checkpointing behavior.

Overall, our results show that the combination of static analysis and lightweight dynamic instrumentation can exclude large volumes of memory that are either unmodified or recomputable, particularly in numerical programs with regular iterative structure.

\subsection{Discussion}

The results support the following observations:

\begin{itemize}
  \item \textbf{Static analysis can yield substantial reductions} in checkpoint size when recomputation is cheap and allocation is disciplined. This aligns with common usage patterns in numerical and data science code.
  \item \textbf{VM- and Process-level diffs greatly overapproximate state necessity}: they include any dirty memory, even if transient or unused, leading to high checkpoint costs.
  \item \textbf{Analysis precision is workload-sensitive}: the benefits are most pronounced when mutable state is well-localized, heap aliasing is limited, and control structure is predictable.
  \item \textbf{Application-level checkpointing improves predictability}: The \spyte method shows more stable checkpoint sizes across runs, with minimal variance and gentler growth rates across iterations compared to system-level approaches.
\end{itemize}

\subsection{Limitations and Threats to Validity}
\label{sec:threats}

Our system assumes that all side effects are either annotated or internal to the function body. If external state is dirty (e.g., global variables, files, sockets) and not captured by the type system, recovery may be unsound.

Additionally, our analysis does not verify behavioral equivalence post-recovery. We produce an explicitly instrumented version of the code so programmers can double-check it themselves. We rely on programmer discipline and the assumption that the relevant control path is idempotent from the checkpoint location onward.

Our analysis takes a few seconds per benchmark function, and can be run entirely ahead-of-time. This is fast enough to be usable in practice and supports iterative development. We do not report fine-grained performance numbers, as the tool is not designed to operate online or scale to large codebases; rather, its value lies in enabling precise transformations in tightly scoped, performance-critical numerical kernels.
