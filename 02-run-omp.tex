
\section{Motivation}
\label{motivation}

\lstset{basicstyle={\ttfamily\fontsize{9pt}{9pt}\selectfont},language=python}

\begin{figure}[t]
\begin{lstlisting}
import numpy as np

def omp(f: np.ndarray, t: np.ndarray, k: int)->np.ndarray:
    S = np.array([], "int")
    for idx in range(k):
        grad, metric = models.oracle(f, t, S)
        points = np.array(range(len(grad)))
        points = np.setdiff1d(points, S).astype("int")
        not points: break
        a = points[0]
        for i in points:
            if grad[i] > grad[a]: a = i
        if grad[a] >= 0 : S = np.append(S,i)
        else: break
\end{lstlisting}
\caption{\label{omp:code}}
\end{figure}

As a running example, we use the OMP feature selection algorithm \cite{mallat1993matching, elenberg2018restricted} shown in \autoref{omp:code}.
Its objective is to select a subset of the feature columns $[1..m]$ from an $n\times m$  feature matrix (X) that most correlates with a target value (Y).
The algorithm performs at most $k$ iterations, in each of which one, best-suited, feature is selected from the set of feature columns.
The selection is based on gradients computed from a regression model (commonly, linear or logistic regression is used).
Based on the gradient, the largest local improvement (max grad[i]) is selected and added to the feature index vector S.

When $k$ is large, many iterations may be required before the algorithm stabilizes (grad[a]${}<0$).
Now, if the program crashes in the middle of the i'th iteration, it can be resumed from the beginning of that iteration by restoring the vector S to its value at that point.
The input (X, Y), the vector S, and the iteration count i is all the internal state needed for the program to reproduce the program behavior from the last checkpoint before the crash.
In this work, we show how this attribute can be leveraged to obtain automated, efficient checkpointing for the program in \autoref{omp:code} and programs of a similar nature.

As a baseline, the best known strategy for systematic checkpointing is machine-level snapshots based on CPU virtualization 
%@moshik add citation to your paper?
\cite{moshik}.
When the program is executed in a VM, a memory snapshot of the VM can be maintained and periodically updated.
However, the resident set of this program is much larger than S, since it also includes the memory utilized by the regression procedure \lstinline|models.oracle(..)|.
Approaches to reducing the storage traffic by flushing only pages or cachelines that are dirty (i.e. were written to since the last snapshot)
would not help in this case, as the regression computation allocates and fills a large scratch space.
What is hidden from such system-level dynamic analysis is that the scratch space is no longer read from after the regression has finished.
In particular, at the checkpoint --- the loop header --- all scratch space is ``dead'', as in,
the data stored then will not be used anymore.
The same is true for the vector \lstinline|grads|, that is being recomputed at every iteration, discarding its previous value.
